---
layout: "post"
title: "BERT 논문정리"
date: "2018-12-12 20:14"
category: NLP
tag: [NLP, 논문정, bert, transformer]
---

# BERT: Pre-trainig of Deep Bidirectional Transformers for Language Understanding

- 최근에 NLP 연구분야에서 핫한 모델인 BERT 논문을 읽고 정리하는 포스트
- 구성은 논문을 쭉 읽어나가며 정리한 포스트라 논문과 같은 순서로 정리함

## Abstract

- BERT : Bidirectional Encoder Representations form Transformer
  - 논문의 제목에서 볼 수 있듯이, 본 논문은 *"Attention is all you need(Vaswani et al., 2017)"*([arxiv](https://arxiv.org/abs/1706.03762))에서 소개한 **Transformer** 구조를 활용한 **Language Representation**에 관한 논문이다.
  - **Transformer**에 대한 자세한 구조를 알고 싶은 분은 위 논문을 읽어보시거나, 다음 블로그 [MChromiak's blog](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/#.XBDvwBMzbOT)를 참고하시면 좋을 듯 하다.
- BERT는 기본적으로, wiki나 book data와 같은 대용랑 **unlabeled data**로 모델을 미리 학습 시킨 후, 특정 *task*를 가지고 있는 *labeled data*로 **transfer learning**을 하는 모델이다.
- BERT이전에 이러한 접근 방법을 가지는 모델이 몇가지 있었다. 대용량 unlabeld corpus를 통해 **language model**을 학습하고, 이를 토대로 뒤쪽에 특정 *task*를 처리하는 network를 붙이는 방식...(ELMo, OpenAI GPT...)
- 하지만 BERT 논문에서는 이전의 모델의 접근 방식은 **shallow bidirectional** 또는 **unidirectional**하므로 language representation이 부족하다고 표현함
- 게다가 BERT는 특정 task를 처리하기 위해 새로운 network를 붙일 필요 없이, BERT 모델 자체의 **fine-tuning**을 통해 해당 task의 *state-of-the-art*를 달성했다고 한다.

## 1. Introduction
- Introduction에서는 BERT와 비슷한 접근 방식을 가지고 있는 기존 model에 대한 개략적인 소개를 한다.

---

- Language model pre-training은 여러 NLP task의 성능을 향상시키는데에 탁월한 효과가 있다고 알려져 있다. (Dai and Le, 2015; Peters et al., 2018, 2018; Radford et al., 2018; ...)
- 이러한 NLP task는 token-level task인 Named Entity Recognition(NER)에서부터 [SQuAD question answering task](https://arxiv.org/abs/1606.05250)와 같은 task까지 광범위한 부분을 커버함
- 이런 **pre-trained language representation**을 적용하는 방식은 크게 두가지 방식이 있음 하나는 **feature-based** 또 다른 하나는 **fine-tuning** 방식
  - **feature-based approach** : 특정 task를 수행하는 network에 pre-trained language representation을 추가적인 feature로 제공. 즉, 두 개의 network를 붙여서 사용한다고 보면 됨. 대표적인 모델 : ELMo([Peters et al., 2018](https://arxiv.org/abs/1802.05365))
  - **fine-tuning approach** : task-specific한 parameter를 최대한 줄이고, pre-trained된 parameter들을 downstream task 학습을 통해 조금만 바꿔주는(fine-tuning) 방식. 대표적인 모델 : Generative Pre-trained Transformer(OpenAI GPT) ([Radford et al., 2018](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf))
- 앞에 소개한 ELMo, OpenAI GPT는 pre-training시에 동일한 **objective funtion**으로 학습을 수행함, 하지만 **BERT**는 새로운 방식으로 **pre-trained Language Representation**을 학습했고 이것은 매우 효과적이었다.

### BERT의 pre-training 방법론
![그림1. BERT, GPT, ELMo (출처 : BERT 논문)](https://mino-park7.github.io/images/2018/12/그림1-bert-openai-gpt-elmo-출처-bert논문.png)
그림1. BERT, GPT, ELMo (출처 : BERT 논문)
- **BERT** pre-training의 새로운 방법론은 크게 2가지로 나눈다. 하나는 **Masked Language Model(MLM)**, 또 다른 하나는 **next sentence prediction**이다.
  - 기존 방법론 : 앞에 소개한 ELMo, OpenAI GPT는 일반적인 language model을 사용하였다. 일반적인 language model이란, 앞의 n 개의 단어를 가지고 뒤의 단어를 예측하는 모델을 세우는 것이다(n-gram). 하지만 이는 필연적으로 **unidirectional**할 수 밖에 없고, 이러한 단점을 극복하기 위해 **ELMo**에서는 **Bi-LSTM**으로 양방향성을 가지려고 노력하지만, 굉장히 **shallow**한 양방향성 (단방향 concat 단방향)만을 가질 수 밖에 없었음(*그림1*).
  - **Masked Language Model(MLM)** : MLM은 input에서 무작위하게 몇개의 token을 mask 시킨다. 그리고 이를 **Transformer** 구조에 넣어서 주변 단어의 context만을 보고 mask된 단어를 예측하는 모델이다. **OpenAI GPT**도 **Transformer** 구조를 사용하지만, 앞의 단어들만 보고 뒷 단어를 예측하는 **Transformer decoder**구조를 사용한다(*그림1*). 이와 달리 **BERT**에서는 input 전체와 mask된 token을 한번에 **Transformer encoder**에 넣고 **decoder**를 통해 원래 token 값을 예측하므로(*그림1*) **deep bidirectional** 하다고 할 수 있다. BERT의 MLM에 대해서는 뒷장에서 더 자세히 설명
  - next sentence prediction : 이것은 간단하게, 두 문장을 pre-training시에 같이 넣어줘서 두 문장이 이어지는 문장인지 아닌지 맞추는 것. pre-training시에는 50:50 비율로 실제로 이어지는 두 문장과 랜덤하게 추출된 두 문장을 넣어줘서 BERT가 맞추게 시킴. 이러한 task는 실제 Natural Language Inference와 같은 task를 수행할 때 도움이 됨

## 2. Related Work
- ELMo, OpenAI GPT와 같은 모델이 존재함... 앞에서 충분히 소개하여 skip

## 3. BERT
- BERT의 아키텍처는 **Attention is all you need**에서 소개된 **Transformer**를 사용하지만, pre-training과 fine-tuning시의 아키텍처를 조금 다르게하여 **Transfer Learning**을 용이하게 만드는 것이 핵심

### 3.1 Model Architecture

- **BERT**는 *transformer* 중에서도 **encoder** 부분만을 사용합니다. 이에 대한 자세한 내용은 [Vaswani et al (2017)](https://arxiv.org/abs/1706.03762) 또는 [tensor2tensor](https://github.com/tensorflow/tensor2tensor)를 참고 바랍니다.
- **BERT**는 모델의 크기에 따라 *base* 모델과 *large* 모델을 제공합니다.
  - **BERT_base** : L=12, H=768, A=12, Total Parameters = 110M
  - **BERT_large** : L=24, H=1024, A=16, Total Parameters = 340M
  - L : transformer block의 layer 수, H : hidden size, A : self-attention heads 수, feed-forward/filter size = 4H
- 여기서 **BERT_base** 모델의 경우, **OpenAI GPT**모델과 *hyper parameter*가 **동일**합니다. 여기서 BERT의 저자가 의도한 바는, 모델의 하이퍼 파라미터가 동일하더라도, **pre-training concept**를 바꾸어 주는 것만으로 훨씬 높은 성능을 낼 수 있다는 것을 보여주고자 하는 것 같습니다.
- **OpenAI GPT**모델의 경우 *그림1*에서 볼 수 있듯, next token 만을 맞추어내는 기본적인 *language model* 방식을 사용하였고, 그를 위해 **transformer decoder**를 사용했습니다. 하지만 **BERT**는 **MLM**과 **NSP**를 위해 **self-attention**을 수행하는 **transformer encoder**구조를 사용했음을 알 수 있습니다.
- 실제로 대부분의 **NLP task SOTA**는 **BERT_large**모델로 이루어 냈습니다.

### 3.2 Input Representation

![그림2. bert input representation (출처: BERT 논문)](https://mino-park7.github.io/images/2019/02/bert-input-representation.png)

- BERT의 input은 *그림 2*와 같이 3가지 embedding 값의 합으로 이루어져 있습니다.
- **WordPiece embedding**을 사용합니다. [WordPiece(Wu et al., 2016)](https://arxiv.org/abs/1609.08144)에 대한 자세한 내용은 논문 링크를 참고 하시거나, [lovit님의 블로그글](https://lovit.github.io/nlp/2018/04/02/wpm/)을 참고 바랍니다. BERT english의 경우 30000개의 token을 사용하였습니다.
- *그림 2*에서 볼 수 있듯이, *Position embedding*을 사용합니다. 이는 **Transformer**에서 사용한 방식과 같으며, [jalammer의 블로그 글](http://jalammar.github.io/illustrated-transformer/)을 참고하시면 position embedding 뿐만 아니라 transformer의 전체적인 구조를 이해 하실 수 있습니다.
- 모든 sentence의 첫번째 token은 언제나 `[CLS]`(special classification token) 입니다. 이 `[CLS]` token은 transformer 전체층을 다 거치고 나면 token sequence의 결합된 의미를 가지게 되는데, 여기에 **간단한 classifier를 붙이면 단일 문장, 또는 연속된 문장의 classification을 쉽게 할 수 있게 됩니다**. 만약 classification task가 아니라면 이 token은 무시하면 됩니다.
- **Sentence pair는 합쳐져서 single sequence**로 입력되게 됩니다. 각각의 Sentence는 실제로는 수 개의 sentence로 이루어져 있을 수 있습니다(eg. QA task의 경우 `[Question, Paragraph]`에서 Paragraph가 여러개의 문장). 그래서 두 개의 문장을 구분하기 위해, 첫째로는 **`[SEP]` token 사용**, 둘째로는 **Segment embedding**을 사용하여 앞의 문장에는 `sentence A embedding`, 뒤의 문장에는 `sentence B embedding`을 더해줍니다.(모두 고정된 값)
- 만약 문장이 하나만 들어간다면 `sentence A embedding`만을 사용합니다.
