---
layout: "post"
title: "BERT 논문정리"
date: "2018-12-12 20:14"
category: NLP
tag: [NLP, 논문정, bert, transformer]
---

# BERT: Pre-trainig of Deep Bidirectional Transformers for Language Understanding

- 최근에 NLP 연구분야에서 핫한 모델인 BERT 논문을 읽고 정리하는 포스트
- 구성은 논문을 쭉 읽어나가며 정리한 포스트라 논문과 같은 순서로 정리함

## Abstract

- BERT : Bidirectional Encoder Representations form Transformer
  - 논문의 제목에서 볼 수 있듯이, 본 논문은 *"Attention is all you need(Vaswani et al., 2017)"*([arxiv](https://arxiv.org/abs/1706.03762))에서 소개한 **Transformer** 구조를 활용한 **Language Representation**에 관한 논문이다.
  - **Transformer**에 대한 자세한 구조를 알고 싶은 분은 위 논문을 읽어보시거나, 다음 블로그 [MChromiak's blog](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/#.XBDvwBMzbOT)를 참고하시면 좋을 듯 하다.
- BERT는 기본적으로, wiki나 book data와 같은 대용랑 **unlabeled data**로 모델을 미리 학습 시킨 후, 특정 *task*를 가지고 있는 *labeled data*로 **transfer learning**을 하는 모델이다.
- BERT이전에 이러한 접근 방법을 가지는 모델이 몇가지 있었다. 대용량 unlabeld corpus를 통해 **language model**을 학습하고, 이를 토대로 뒤쪽에 특정 *task*를 처리하는 network를 붙이는 방식...(ELMo, OpenAI GPT...)
- 하지만 BERT 논문에서는 이전의 모델의 접근 방식은 **shallow bidirectional** 또는 **unidirectional**하므로 language representation이 부족하다고 표현함
- 게다가 BERT는 특정 task를 처리하기 위해 새로운 network를 붙일 필요 없이, BERT 모델 자체의 **fine-tuning**을 통해 해당 task의 *state-of-the-art*를 달성했다고 한다.

## 1. Introduction
- Introduction에서는 BERT와 비슷한 접근 방식을 가지고 있는 기존 model에 대한 개략적인 소개를 한다.

---

- Language model pre-training은 여러 NLP task의 성능을 향상시키는데에 탁월한 효과가 있다고 알려져 있다. (Dai and Le, 2015; Peters et al., 2018, 2018; Radford et al., 2018; ...)
- 이러한 NLP task는 token-level task인 Named Entity Recognition(NER)에서부터 [SQuAD question answering task](https://arxiv.org/abs/1606.05250)와 같은 task까지 광범위한 부분을 커버함
- 이런 **pre-trained language representation**을 적용하는 방식은 크게 두가지 방식이 있음 하나는 **feature-based** 또 다른 하나는 **fine-tuning** 방식
  - **feature-based approach** : 특정 task를 수행하는 network에 pre-trained language representation을 추가적인 feature로 제공. 즉, 두 개의 network를 붙여서 사용한다고 보면 됨. 대표적인 모델 : ELMo([Peters et al., 2018](https://arxiv.org/abs/1802.05365))
  - **fine-tuning approach** : task-specific한 parameter를 최대한 줄이고, pre-trained된 parameter들을 downstream task 학습을 통해 조금만 바꿔주는(fine-tuning) 방식. 대표적인 모델 : Generative Pre-trained Transformer(OpenAI GPT) ([Radford et al., 2018](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf))
- 앞에 소개한 ELMo, OpenAI GPT는 pre-training시에 동일한 **objective funtion**으로 학습을 수행함, 하지만 **BERT**는 새로운 방식으로 **pre-trained Language Representation**을 학습했고 이것은 매우 효과적이었다.

### BERT의 pre-training 방법론
![그림1. BERT, GPT, ELMo (출처 : BERT 논문)](/images/2018/12/그림1-bert-openai-gpt-elmo-출처-bert논문.png)
그림1. BERT, GPT, ELMo (출처 : BERT 논문)
- **BERT** pre-training의 새로운 방법론은 크게 2가지로 나눈다. 하나는 **Masked Language Model(MLM)**, 또 다른 하나는 **next sentence prediction**이다.
  - 기존 방법론 : 앞에 소개한 ELMo, OpenAI GPT는 일반적인 language model을 사용하였다. 일반적인 language model이란, 앞의 n 개의 단어를 가지고 뒤의 단어를 예측하는 모델을 세우는 것이다(n-gram). 하지만 이는 필연적으로 **unidirectional**할 수 밖에 없고, 이러한 단점을 극복하기 위해 **ELMo**에서는 **Bi-LSTM**으로 양방향성을 가지려고 노력하지만, 굉장히 **shallow**한 양방향성 (단방향 concat 단방향)만을 가질 수 밖에 없었음(*그림1*).
  - **Masked Language Model(MLM)** : MLM은 input에서 무작위하게 몇개의 token을 mask 시킨다. 그리고 이를 **Transformer** 구조에 넣어서 주변 단어의 context만을 보고 mask된 단어를 예측하는 모델이다. **OpenAI GPT**도 **Transformer** 구조를 사용하지만, 앞의 단어들만 보고 뒷 단어를 예측하는 **Transformer decoder**구조를 사용한다(*그림1*). 이와 달리 **BERT**에서는 input 전체와 mask된 token을 한번에 **Transformer encoder**에 넣고 **decoder**를 통해 원래 token 값을 예측하므로(*그림1*) **deep bidirectional** 하다고 할 수 있다. BERT의 MLM에 대해서는 뒷장에서 더 자세히 설명
  - next sentence prediction : 이것은 간단하게, 두 문장을 pre-training시에 같이 넣어줘서 두 문장이 이어지는 문장인지 아닌지 맞추는 것. pre-training시에는 50:50 비율로 실제로 이어지는 두 문장과 랜덤하게 추출된 두 문장을 넣어줘서 BERT가 맞추게 시킴. 이러한 task는 실제 Natural Language Inference와 같은 task를 수행할 때 도움이 됨

## 2. Related Work
- ELMo, OpenAI GPT와 같은 모델이 존재함... 앞에서 충분히 소개하여 skip

## 3. BERT
- BERT의 아키텍처는 **Attention is all you need**에서 소개된 **Transformer**를 사용하지만, pre-training과 fine-tuning시의 아키텍처를 조금 다르게하여 **Transfer Learning**을 용이하게 만드는 것이 핵심

### 3.1 Model Architecture
