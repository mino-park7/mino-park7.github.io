---
layout: "post"
title: "BERT 논문정리"
date: "2018-12-12 20:14"
category: NLP
tag: [NLP, 논문정, bert, transformer]
---

# BERT: Pre-trainig of Deep Bidirectional Transformers for Language Understanding

- 최근에 NLP 연구분야에서 핫한 모델인 BERT 논문을 읽고 정리하는 포스트
- 구성은 논문을 쭉 읽어나가며 정리한 포스트라 논문과 같은 순서로 정리함

## Abstract

- BERT : Bidirectional Encoder Representations form Transformer
  - 논문의 제목에서 볼 수 있듯이, 본 논문은 *"Attention is all you need(Vaswani et al., 2017)"*([arxiv](https://arxiv.org/abs/1706.03762))에서 소개한 **Transformer** 구조를 활용한 **Language Representation**에 관한 논문이다.
  - **Transformer**에 대한 자세한 구조를 알고 싶은 분은 위 논문을 읽어보시거나, 다음 블로그 [MChromiak's blog](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/#.XBDvwBMzbOT)를 참고하시면 좋을 듯 하다.
- BERT는 기본적으로, wiki나 book data와 같은 대용랑 **unlabeled data**로 모델을 미리 학습 시킨 후, 특정 *task*를 가지고 있는 *labeled data*로 **transfer learning**을 하는 모델이다.
- BERT이전에 이러한 접근 방법을 가지는 모델이 몇가지 있었다. 대용량 unlabeld corpus를 통해 **language model**을 학습하고, 이를 토대로 뒤쪽에 특정 *task*를 처리하는 network를 붙이는 방식...(ELMo, OpenAI GPT...)
- 하지만 BERT 논문에서는 이전의 모델의 접근 방식은 **shallow bidirectional** 또는 **unidirectional**하므로 language representation이 부족하다고 표현함
- 게다가 BERT는 특정 task를 처리하기 위해 새로운 network를 붙일 필요 없이, BERT 모델 자체의 **fine-tuning**을 통해 해당 task의 *state-of-the-art*를 달성했다고 한다.

## Introduction
- Introduction에서는 BERT와 비슷한 접근 방식을 가지고 있는 기존 model에 대한 개략적인 소개를 한다.

---

- Language model pre-training은 여러 NLP task의 성능을 향상시키는데에 탁월한 효과가 있다고 알려져 있다. (Dai and Le, 2015; Peters et al., 2018, 2018; Radford et al., 2018; ...)
- 이러한 NLP task는 token-level task인 Named Entity Recognition(NER)에서부터 [SQuAD question answering task](https://arxiv.org/abs/1606.05250)와 같은 task까지 광범위한 부분을 커버함
- 이런 **pre-trained language representation**을 적용하는 방식은 크게 두가지 방식이 있음 하나는 **feature-based** 또 다른 하나는 **fine-tuning** 방식
  - **feature-based approach** : 특정 task를 수행하는 network에 pre-trained language representation을 추가적인 feature로 제공. 즉, 두 개의 network를 붙여서 사용한다고 보면 됨. 대표적인 모델 : ELMo([Peters et al., 2018](https://arxiv.org/abs/1802.05365))
  - **fine-tuning approach** : task-specific한 parameter를 최대한 줄이고, pre-trained된 parameter들을 downstream task 학습을 통해 조금만 바꿔주는(fine-tuning) 방식. 대표적인 모델 : Generative Pre-trained Transformer(OpenAI GPT) ([Radford et al., 2018](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf))
- 앞에 소개한 ELMo, OpenAI GPT는 pre-training시에 동일한 **objective funtion**으로 학습을 수행함, 하지만 **BERT**는 새로운 방식으로 **pre-trained Language Representation**을 학습했고 이것은 매우 효과적이었다.
