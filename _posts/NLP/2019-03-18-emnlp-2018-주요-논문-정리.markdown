---
layout: "post"
title: "EMNLP 2018 주요 논문 정리"
date: "2019-03-18 11:32"
category: NLP
tag: [NLP, EMNLP, 학회정리, 논문정리]
---





# EMNLP 2018

- 이번 포스트에서는 2018년 10월 31일 부터 11월 4일까지 열린 EMNLP의 주요 논문에 대한 정리를 합니다.
- 본 포스트는 [sebastian Ruder의 블로그](http://ruder.io/emnlp-2018-highlights/)를 참고하여 작성하였음을 알려드립니다.
- EMNLP의 페이퍼들은 다음의 **key topic**을 가지고 있다고 얘기 할 수 있습니다.
  - Inductive bias
  - Cross-lingual Learning
  - Word embedding
  - Latent variable models
  - Language models
  - Datasets
  - Miscellaneous

- 각각에 대해 정리를 하도록 하겠습니다.

## 1. Inductive bias (귀납적 편중...?)
- 일단, bias란 무엇일까요?
  - 굉장히 여러 의미로 사용 될 수 있지만, **편견**이나 **offset**과 비슷한 의미입니다.
- 여기서 inductive, 즉 **귀납적**이라는 말이 추가로 붙게 됩니다.
- 기계학습에서의 inductive bias는, 학습 모델이 **지금까지 만나보지 못했던 상황**에서 정확한 예측을 하기 위해 사용하는 **추가적인 가정**을 의미합니다. ([Wikipedia](https://en.wikipedia.org/wiki/Inductive_bias))
- 기계학습에서는 특정 **목표 출력**(target output)을 예측하기 위해 **학습 가능한** 알고리즘 구축을 목표로 하고, 학습 모델에는 **제한된 수의** 입력과 출력에 대한 data가 주어집니다.
- 학습이 성공적으로 끝난 후에, 학습 모델은 훈련동안에는 보이지 않았던 예들 까지도 정확한 출력에 가까워지도록 추측해야합니다. 이럴 때 **추가적인 가정**이 없이는 불가능한데, 이 **Target function**의 성질에 대해 **필요한 가정**과 같은 것이 **Inductive bias**라고 볼 수 있습니다.
- 가장 고전적이고 대표적인 예가 **Occam's Razor**, 오컴의 면도날입니다. 이는 target function에 대해 가장 단순한 무모순의 가설이 실제로 가장 좋을 것이라고 가정합니다.
---
- 이 inductive bias의 개념은 [Max Welling의 CoNLL 2018 keynote](http://www.conll.org/keynotes-2018)에서의 메인 테마였습니다. 2가지의 비결은 다음과 같습니다.

### 1.1 Exploit Symmetry
- 만약 **input space**에 **대칭성**이 존재한다면, 그것을 사용하라는 것입니다.
- 이 대칭성에 관한 가장 좋은 예제가 **CNN**입니다.
  - 기본적인 CNN의 경우, 아래 그림과 같이 **Translation Invariance**, [Steerable CNN](https://arxiv.org/abs/1612.08498)의 겅우 **Rotation/Viewpoint Invariance한 특징**을 가집니다.
  - 이러한 invariance를 가지는 것은 **우리**가 특정한 **inductive bias**를 가지고 CNN 구조를 만들었기 때문에 가능한 것입니다.
- 그래서 우리는 **자연어**가 어써한 타입의 **invariance**를 가질지, 그리고 그것은 어떻게 **neural network**에 적용할 수 있을지에 대한 연구를 해야합니다.

![Translation and rotation invariance in computer vision (source: Matt Krause)](/images/2019/03/translation-and-rotation-invariance-in-computer-vision-source-matt-krause.png)
fig1. Translation and rotation invariance in computer vision (source: Matt Krause)

### 1.2 Exploit generative process
-
